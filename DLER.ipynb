{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import scipy.io\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "/SEED-V.zip is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(data_path)\n\u001b[1;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/SEED-V.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m shutil\u001b[38;5;241m.\u001b[39munpack_archive(filename, data_path)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/shutil.py:1241\u001b[0m, in \u001b[0;36munpack_archive\u001b[0;34m(filename, extract_dir, format)\u001b[0m\n\u001b[1;32m   1239\u001b[0m func \u001b[39m=\u001b[39m _UNPACK_FORMATS[\u001b[39mformat\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m   1240\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(_UNPACK_FORMATS[\u001b[39mformat\u001b[39m][\u001b[39m2\u001b[39m])\n\u001b[0;32m-> 1241\u001b[0m func(filename, extract_dir, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/shutil.py:1136\u001b[0m, in \u001b[0;36m_unpack_zipfile\u001b[0;34m(filename, extract_dir)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m  \u001b[39m# late import for breaking circular dependency\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m zipfile\u001b[39m.\u001b[39mis_zipfile(filename):\n\u001b[0;32m-> 1136\u001b[0m     \u001b[39mraise\u001b[39;00m ReadError(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not a zip file\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m filename)\n\u001b[1;32m   1138\u001b[0m \u001b[39mzip\u001b[39m \u001b[39m=\u001b[39m zipfile\u001b[39m.\u001b[39mZipFile(filename)\n\u001b[1;32m   1139\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mReadError\u001b[0m: /SEED-V.zip is not a zip file"
     ]
    }
   ],
   "source": [
    "data_path = './Dataset/'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "filename = '/SEED-V.zip'\n",
    "shutil.unpack_archive(filename, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and label from npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'label']\n"
     ]
    }
   ],
   "source": [
    "data_npz = np.load('/Users/calebjonesshibu/Documents/UofA/Classes/Sem3/Thesis/deep-learning-emotion-recognition/Dataset/SEED-V/EEG_DE_features/1_123.npz')\n",
    "print(data_npz.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])\n"
     ]
    }
   ],
   "source": [
    "data = pickle.loads(data_npz['data'])\n",
    "label = pickle.loads(data_npz['label'])\n",
    "\n",
    "print(data.keys())\n",
    "print(label.keys())\n",
    "\n",
    "# As we can see, there are 45 keys in both 'data' and 'label'.\n",
    "# Each participant took part in our experiments for 3 sessions, and he/she watched 15 movie clips (i.e. 15 trials) during each session.\n",
    "# Therefore, we could extract 3 * 15 = 45 DE feature matrices.\n",
    "\n",
    "# The key indexes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] belong to Session 1.\n",
    "# The key indexes [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] belong to Session 2.\n",
    "# The key indexes [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44] belong to Session 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 -- Trial 1 -- EmotionLabel : Happy\n",
      "Session 1 -- Trial 2 -- EmotionLabel : Fear\n",
      "Session 1 -- Trial 3 -- EmotionLabel : Neutral\n",
      "Session 1 -- Trial 4 -- EmotionLabel : Sad\n",
      "Session 1 -- Trial 5 -- EmotionLabel : Disgust\n",
      "Session 1 -- Trial 6 -- EmotionLabel : Happy\n",
      "Session 1 -- Trial 7 -- EmotionLabel : Fear\n",
      "Session 1 -- Trial 8 -- EmotionLabel : Neutral\n",
      "Session 1 -- Trial 9 -- EmotionLabel : Sad\n",
      "Session 1 -- Trial 10 -- EmotionLabel : Disgust\n",
      "Session 1 -- Trial 11 -- EmotionLabel : Happy\n",
      "Session 1 -- Trial 12 -- EmotionLabel : Fear\n",
      "Session 1 -- Trial 13 -- EmotionLabel : Neutral\n",
      "Session 1 -- Trial 14 -- EmotionLabel : Sad\n",
      "Session 1 -- Trial 15 -- EmotionLabel : Disgust\n",
      "Session 2 -- Trial 1 -- EmotionLabel : Sad\n",
      "Session 2 -- Trial 2 -- EmotionLabel : Fear\n",
      "Session 2 -- Trial 3 -- EmotionLabel : Neutral\n",
      "Session 2 -- Trial 4 -- EmotionLabel : Disgust\n",
      "Session 2 -- Trial 5 -- EmotionLabel : Happy\n",
      "Session 2 -- Trial 6 -- EmotionLabel : Happy\n",
      "Session 2 -- Trial 7 -- EmotionLabel : Disgust\n",
      "Session 2 -- Trial 8 -- EmotionLabel : Neutral\n",
      "Session 2 -- Trial 9 -- EmotionLabel : Sad\n",
      "Session 2 -- Trial 10 -- EmotionLabel : Fear\n",
      "Session 2 -- Trial 11 -- EmotionLabel : Neutral\n",
      "Session 2 -- Trial 12 -- EmotionLabel : Happy\n",
      "Session 2 -- Trial 13 -- EmotionLabel : Fear\n",
      "Session 2 -- Trial 14 -- EmotionLabel : Sad\n",
      "Session 2 -- Trial 15 -- EmotionLabel : Disgust\n",
      "Session 3 -- Trial 1 -- EmotionLabel : Sad\n",
      "Session 3 -- Trial 2 -- EmotionLabel : Fear\n",
      "Session 3 -- Trial 3 -- EmotionLabel : Neutral\n",
      "Session 3 -- Trial 4 -- EmotionLabel : Disgust\n",
      "Session 3 -- Trial 5 -- EmotionLabel : Happy\n",
      "Session 3 -- Trial 6 -- EmotionLabel : Happy\n",
      "Session 3 -- Trial 7 -- EmotionLabel : Disgust\n",
      "Session 3 -- Trial 8 -- EmotionLabel : Neutral\n",
      "Session 3 -- Trial 9 -- EmotionLabel : Sad\n",
      "Session 3 -- Trial 10 -- EmotionLabel : Fear\n",
      "Session 3 -- Trial 11 -- EmotionLabel : Neutral\n",
      "Session 3 -- Trial 12 -- EmotionLabel : Happy\n",
      "Session 3 -- Trial 13 -- EmotionLabel : Fear\n",
      "Session 3 -- Trial 14 -- EmotionLabel : Sad\n",
      "Session 3 -- Trial 15 -- EmotionLabel : Disgust\n"
     ]
    }
   ],
   "source": [
    "# We will print the emotion labels for each trial.\n",
    "label_dict = {0:'Disgust', 1:'Fear', 2:'Sad', 3:'Neutral', 4:'Happy'}\n",
    "for i in range(45):\n",
    "    print('Session {} -- Trial {} -- EmotionLabel : {}'.format(i//15+1, i%15+1, label_dict[label[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 45)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 310)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data as train test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_cv_data(eeg_dir, eye_dir, file_name, cv_number):\n",
    "    eeg_data_pickle = np.load( os.path.join(eeg_dir, file_name))\n",
    "    eye_data_pickle = np.load( os.path.join(eye_dir, file_name))\n",
    "    eeg_data = pickle.loads(eeg_data_pickle['data'])\n",
    "    eye_data = pickle.loads(eye_data_pickle['data'])\n",
    "    label = pickle.loads(eeg_data_pickle['label'])\n",
    "    list_1 = [0,1,2,3,4,15,16,17,18,19,30,31,32,33,34]\n",
    "    list_2 = [5,6,7,8,9,20,21,22,23,24,35,36,37,38,39]\n",
    "    list_3 = [10,11,12,13,14,25,26,27,28,29,40,41,42,43,44]\n",
    "    if cv_number == 1:\n",
    "        print('#1 as test, preparing data')\n",
    "        train_list = list_2 + list_3\n",
    "        test_list = list_1\n",
    "    elif cv_number == 2:\n",
    "        print('#2 as test, preparing data')\n",
    "        train_list = list_1 + list_3\n",
    "        test_list = list_2\n",
    "    else:\n",
    "        print('#3 as test, preparing data')\n",
    "        train_list = list_1 + list_2\n",
    "        test_list = list_3\n",
    "\n",
    "    train_eeg = []\n",
    "    test_eeg = []\n",
    "    train_label = []\n",
    "    for train_id in range(len(train_list)):\n",
    "        train_eeg_tmp = eeg_data[train_list[train_id]]\n",
    "        train_eye_tmp = eye_data[train_list[train_id]]\n",
    "        train_label_tmp = label[train_list[train_id]]\n",
    "        if train_id == 0:\n",
    "            train_eeg = train_eeg_tmp\n",
    "            train_eye = train_eye_tmp\n",
    "            train_label = train_label_tmp\n",
    "        else:\n",
    "            train_eeg = np.vstack((train_eeg, train_eeg_tmp))\n",
    "            train_eye = np.vstack((train_eye, train_eye_tmp))\n",
    "            train_label = np.hstack((train_label, train_label_tmp))\n",
    "    assert train_eeg.shape[0] == train_eye.shape[0]\n",
    "    assert train_eeg.shape[0] == train_label.shape[0]\n",
    "\n",
    "    test_eeg = []\n",
    "    test_eye = []\n",
    "    test_label = []\n",
    "    for test_id in range(len(test_list)):\n",
    "        test_eeg_tmp = eeg_data[test_list[test_id]]\n",
    "        test_eye_tmp = eye_data[test_list[test_id]]\n",
    "        test_label_tmp = label[test_list[test_id]]\n",
    "        if test_id == 0:\n",
    "            test_eeg = test_eeg_tmp\n",
    "            test_eye = test_eye_tmp\n",
    "            test_label = test_label_tmp\n",
    "        else:\n",
    "            test_eeg = np.vstack((test_eeg, test_eeg_tmp))\n",
    "            test_eye = np.vstack((test_eye, test_eye_tmp))\n",
    "            test_label = np.hstack((test_label, test_label_tmp))\n",
    "    assert test_eeg.shape[0] == test_eye.shape[0]\n",
    "    assert test_eeg.shape[0] == test_label.shape[0]\n",
    "\n",
    "    train_all = np.hstack((train_eeg, train_eye, train_label.reshape([-1,1])))\n",
    "    test_all = np.hstack((test_eeg, test_eye, test_label.reshape([-1,1])))\n",
    "    return train_all, test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dir = '/Users/calebjonesshibu/Documents/UofA/Classes/Sem3/Thesis/deep-learning-emotion-recognition/Dataset/SEED-V/EEG_DE_features/'\n",
    "eye_dir = '/Users/calebjonesshibu/Documents/UofA/Classes/Sem3/Thesis/deep-learning-emotion-recognition/Dataset/SEED-V/Eye_movement_features/'\n",
    "file_list = os.listdir(eeg_dir)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parts of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cca_metric_derivative(H1, H2):\n",
    "    r1 = 1e-3\n",
    "    r2 = 1e-3\n",
    "    eps = 1e-9\n",
    "    # transform the matrix: to be consistent with the original paper\n",
    "    H1 = H1.T\n",
    "    H2 = H2.T\n",
    "    # o1 and o2 are feature dimensions\n",
    "    # m is sample number\n",
    "    o1 = o2 = H1.shape[0]\n",
    "    m = H1.shape[1]\n",
    "\n",
    "    # calculate parameters\n",
    "    H1bar = H1 - H1.mean(axis=1).reshape([-1,1])\n",
    "    H2bar = H2 - H2.mean(axis=1).reshape([-1,1])\n",
    "\n",
    "    SigmaHat12 = (1.0 / (m - 1)) * np.matmul(H1bar, H2bar.T)\n",
    "    SigmaHat11 = (1.0 / (m - 1)) * np.matmul(H1bar, H1bar.T) + r1 * np.eye(o1)\n",
    "    SigmaHat22 = (1.0 / (m - 1)) * np.matmul(H2bar, H2bar.T) + r2 * np.eye(o2)\n",
    "\n",
    "    # eigenvalue and eigenvector decomposition\n",
    "    [D1, V1] = np.linalg.eigh(SigmaHat11)\n",
    "    [D2, V2] = np.linalg.eigh(SigmaHat22)\n",
    "\n",
    "    # remove eighvalues and eigenvectors smaller than 0\n",
    "    posInd1 = np.where(D1 > 0)[0]\n",
    "    D1 = D1[posInd1]\n",
    "    V1 = V1[:, posInd1]\n",
    "\n",
    "    posInd2 = np.where(D2 > 0)[0]\n",
    "    D2 = D2[posInd2]\n",
    "    V2 = V2[:, posInd2]\n",
    "\n",
    "    # calculate matrxi T\n",
    "    SigmaHat11RootInv = np.matmul(np.matmul(V1, np.diag(D1 ** -0.5)), V1.T)\n",
    "    SigmaHat22RootInv = np.matmul(np.matmul(V2, np.diag(D2 ** -0.5)), V2.T)\n",
    "    Tval = np.matmul(np.matmul(SigmaHat11RootInv,SigmaHat12), SigmaHat22RootInv)\n",
    "    # By default, we will use all the singular values\n",
    "    tmp = np.matmul(Tval.T, Tval)\n",
    "    corr = np.sqrt(np.trace(tmp))\n",
    "    cca_loss = -1 * corr\n",
    "\n",
    "    # calculate the derivative of H1 and H2\n",
    "    U_t, D_t, V_prime_t = np.linalg.svd(Tval)\n",
    "    Delta12 = SigmaHat11RootInv @ U_t @ V_prime_t @ SigmaHat22RootInv\n",
    "    Delta11 = SigmaHat11RootInv @ U_t @ np.diag(D_t) @ U_t.T @ SigmaHat11RootInv\n",
    "    Delta22 = SigmaHat22RootInv @ U_t @ np.diag(D_t) @ U_t.T @ SigmaHat22RootInv\n",
    "    Delta11 = -0.5 * Delta11\n",
    "    Delta22 = -0.5 * Delta22\n",
    "\n",
    "    DerivativeH1 = ( 1.0 / (m - 1)) * (2 * (Delta11 @ H1bar) + Delta12 @ H2bar)\n",
    "    DerivativeH2 = ( 1.0 / (m - 1)) * (2 * (Delta22 @ H2bar) + Delta12 @ H1bar)\n",
    "\n",
    "    return cca_loss, DerivativeH1.T, DerivativeH2.T\n",
    "    \n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(AttentionFusion, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention_weights = nn.Parameter(torch.randn(self.output_dim, requires_grad=True))\n",
    "    def forward(self, x1, x2):\n",
    "        # calculate weigths for all input samples\n",
    "        row, _ = x1.shape\n",
    "        fused_tensor = torch.empty_like(x1)\n",
    "        alpha = []\n",
    "        for i in range(row):\n",
    "            tmp1 = torch.dot(x1[i,:], self.attention_weights)\n",
    "            tmp2 = torch.dot(x2[i,:], self.attention_weights)\n",
    "            alpha_1 = torch.exp(tmp1) / (torch.exp(tmp1) + torch.exp(tmp2))\n",
    "            alpha_2 = 1 - alpha_1\n",
    "            alpha.append((alpha_1.detach().cpu().numpy(), alpha_2.detach().cpu().numpy()))\n",
    "            fused_tensor[i, :] = alpha_1 * x1[i,:] + alpha_2 * x2[i, :]\n",
    "        return fused_tensor, alpha\n",
    "\n",
    "class TransformLayers(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(TransformLayers, self).__init__()\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        for l_id in range(len(layer_sizes) - 1):\n",
    "            if l_id == len(layer_sizes) - 2:\n",
    "                layers.append(nn.Sequential(\n",
    "                    #nn.BatchNorm1d(num_features=layer_sizes[l_id], affine=False),\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id+1]),\n",
    "                    ))\n",
    "            else:\n",
    "                layers.append(nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id+1]),\n",
    "                    nn.Sigmoid(),\n",
    "                    #nn.BatchNorm1d(num_features=layer_sizes[l_id+1], affine=False),\n",
    "                    ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DCCA_AM(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, layer_sizes1, layer_sizes2, outdim_size, categories, device):\n",
    "        super(DCCA_AM, self).__init__()\n",
    "        self.outdim_size = outdim_size\n",
    "        self.categories = categories\n",
    "        # self.use_all_singular_values = use_all_singular_values\n",
    "        self.device = device\n",
    "\n",
    "        self.model1 = TransformLayers(input_size1, layer_sizes1).to(self.device)\n",
    "        self.model2 = TransformLayers(input_size2, layer_sizes2).to(self.device)\n",
    "\n",
    "        self.model1_parameters = self.model1.parameters()\n",
    "        self.model2_parameters = self.model1.parameters()\n",
    "\n",
    "        self.classification = nn.Linear(self.outdim_size, self.categories)\n",
    "\n",
    "        self.attention_fusion = AttentionFusion(outdim_size)\n",
    "    def forward(self, x1, x2):\n",
    "        # forward process: returns negative of cca loss and predicted labels\n",
    "        output1 = self.model1(x1)\n",
    "        output2 = self.model2(x2)\n",
    "        # cca_loss_val = self.loss(output1, output2)\n",
    "        cca_loss, partial_h1, partial_h2 = cca_metric_derivative(output1.detach().cpu().numpy(), output2.detach().cpu().numpy())\n",
    "        fused_tensor, alpha = self.attention_fusion(output1, output2)\n",
    "        out = self.classification(fused_tensor)\n",
    "        return out, cca_loss, output1, output2, partial_h1, partial_h2, fused_tensor.detach().cpu().data, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "emotion_categories = 5\n",
    "\n",
    "epochs = 70\n",
    "eeg_input_dim = 310\n",
    "eye_input_dim = 33\n",
    "output_dim = 12\n",
    "learning_rate = 5 * 1e-4\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_123.npz\n",
      "#3 as test, preparing data\n",
      "[161, 34, 12]\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─TransformLayers: 1-1                   --\n",
      "|    └─ModuleList: 2-1                   --\n",
      "|    |    └─Sequential: 3-1              50,071\n",
      "|    |    └─Sequential: 3-2              5,508\n",
      "|    |    └─Sequential: 3-3              420\n",
      "├─TransformLayers: 1-2                   --\n",
      "|    └─ModuleList: 2-2                   --\n",
      "|    |    └─Sequential: 3-4              5,474\n",
      "|    |    └─Sequential: 3-5              5,508\n",
      "|    |    └─Sequential: 3-6              420\n",
      "├─Linear: 1-3                            65\n",
      "├─AttentionFusion: 1-4                   12\n",
      "=================================================================\n",
      "Total params: 67,478\n",
      "Trainable params: 67,478\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m predict_loss_train \u001b[38;5;241m=\u001b[39m class_loss_func(predict_out_train, train_label)\n\u001b[1;32m     95\u001b[0m accuracy_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39margmax(predict_out_train\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m train_label\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m/\u001b[39m predict_out_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m predict_out_test, cca_loss_test, output_1_test, output_2_test, _, _, fused_tensor_test, attention_weight_test  \u001b[38;5;241m=\u001b[39m mymodel(test_eeg, test_eye)\n\u001b[1;32m     98\u001b[0m predict_loss_test \u001b[38;5;241m=\u001b[39m class_loss_func(predict_out_test, test_label)\n\u001b[1;32m     99\u001b[0m accuracy_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39margmax(predict_out_test\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m test_label\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m/\u001b[39m predict_out_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 122\u001b[0m, in \u001b[0;36mDCCA_AM.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# cca_loss_val = self.loss(output1, output2)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m cca_loss, partial_h1, partial_h2 \u001b[38;5;241m=\u001b[39m cca_metric_derivative(output1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), output2\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 122\u001b[0m fused_tensor, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification(fused_tensor)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, cca_loss, output1, output2, partial_h1, partial_h2, fused_tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata, alpha\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [12], line 68\u001b[0m, in \u001b[0;36mAttentionFusion.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(row):\n\u001b[1;32m     67\u001b[0m     tmp1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(x1[i,:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_weights)\n\u001b[0;32m---> 68\u001b[0m     tmp2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     alpha_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(tmp1) \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39mexp(tmp1) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(tmp2))\n\u001b[1;32m     70\u001b[0m     alpha_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv =3\n",
    "for f_id in file_list:\n",
    "    type(f_id)\n",
    "    logging.basicConfig(filename='./logs/cv3.log', level=logging.DEBUG)\n",
    "    logging.debug('{}'.format(f_id))\n",
    "    logging.debug('Task-Epoch-CCALoss-PredicLoss-PredicAcc')\n",
    "    if f_id.endswith('.npz'):\n",
    "        print(f_id)\n",
    "        train_all, test_all = loading_cv_data(eeg_dir, eye_dir, f_id, cv)\n",
    "        np.random.shuffle(train_all)\n",
    "        np.random.shuffle(test_all)\n",
    "\n",
    "        sample_num = train_all.shape[0]\n",
    "        batch_number = sample_num // batch_size\n",
    "\n",
    "        train_eeg = train_all[:, 0:310]\n",
    "        train_eye = train_all[:, 310:343]\n",
    "        train_label = train_all[:, -1]\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        train_eeg = scaler.fit_transform(train_eeg)\n",
    "        train_eye = scaler.fit_transform(train_eye)\n",
    "        test_eeg = test_all[:, 0:310]\n",
    "        test_eye = test_all[:, 310:343]\n",
    "        test_label = test_all[:, -1]\n",
    "\n",
    "        test_eeg = scaler.fit_transform(test_eeg)\n",
    "        test_eye = scaler.fit_transform(test_eye)\n",
    "\n",
    "        train_eeg = torch.from_numpy(train_eeg).to(torch.float).to(device)\n",
    "        train_eye = torch.from_numpy(train_eye).to(torch.float).to(device)\n",
    "        test_eeg = torch.from_numpy(test_eeg).to(torch.float).to(device)\n",
    "        test_eye = torch.from_numpy(test_eye).to(torch.float).to(device)\n",
    "        train_label = torch.from_numpy(train_label).to(torch.long).to(device)\n",
    "        test_label = torch.from_numpy(test_label).to(torch.long).to(device)\n",
    "\n",
    "        for hyper_choose in range(100):\n",
    "\n",
    "                best_test_res = {}\n",
    "                best_test_res['acc'] = 0\n",
    "                best_test_res['predict_proba'] = None\n",
    "                best_test_res['fused_feature'] = None\n",
    "                best_test_res['transformed_eeg'] = None\n",
    "                best_test_res['transformed_eye'] = None\n",
    "                best_test_res['alpha'] = None\n",
    "                best_test_res['true_label'] = None\n",
    "                best_test_res['layer_size'] = None\n",
    "                # try 100 combinations of different hidden units\n",
    "                layer_sizes = [np.random.randint(100,200), np.random.randint(20,50), output_dim]\n",
    "                logging.info('{}-{}'.format(layer_sizes[0], layer_sizes[1]))\n",
    "                print(layer_sizes)\n",
    "                mymodel = DCCA_AM(eeg_input_dim, eye_input_dim, layer_sizes, layer_sizes, output_dim, emotion_categories, device).to(device)\n",
    "                \n",
    "                summary(mymodel, input_size = eeg_input_dim)                \n",
    "                optimizer_classifier = torch.optim.RMSprop(mymodel.parameters(), lr=learning_rate)\n",
    "                optimizer_model1 = torch.optim.RMSprop(iter(list(mymodel.parameters())[0:8]), lr=learning_rate/2)\n",
    "                optimizer_model2 = torch.optim.RMSprop(iter(list(mymodel.parameters())[8:16]), lr=learning_rate/2)\n",
    "                class_loss_func = nn.CrossEntropyLoss()\n",
    "                for epoch in range(epochs):\n",
    "                    mymodel.train()\n",
    "                    best_acc = 0\n",
    "                    total_classification_loss = 0\n",
    "                    for b_id in range(batch_number+1):\n",
    "                        if b_id == batch_number:\n",
    "                            train_eeg_used = train_eeg[batch_size*batch_number:, :]\n",
    "                            train_eye_used = train_eye[batch_size*batch_number: , :]\n",
    "                            train_label_used = train_label[batch_size*batch_number:]\n",
    "                        else:\n",
    "                            train_eeg_used = train_eeg[b_id*batch_size:(b_id+1)*batch_size, :]\n",
    "                            train_eye_used = train_eye[b_id*batch_size:(b_id+1)*batch_size, :]\n",
    "                            train_label_used = train_label[b_id*batch_size:(b_id+1)*batch_size]\n",
    "\n",
    "                        # predict_out, cca_loss, output1, output2, partial_h1, partial_h2, fused_tensor, transformed_1, transformed_2, alpha  = mymodel(train_eeg_used, train_eye_used)\n",
    "                        predict_out, cca_loss, output1, output2, partial_h1, partial_h2, fused_tensor, alpha  = mymodel(train_eeg_used, train_eye_used)\n",
    "                        predict_loss = class_loss_func(predict_out, train_label_used)\n",
    "\n",
    "                        optimizer_model1.zero_grad()\n",
    "                        optimizer_model2.zero_grad()\n",
    "                        optimizer_classifier.zero_grad()\n",
    "\n",
    "                        partial_h1 = torch.from_numpy(partial_h1).to(torch.float).to(device)\n",
    "                        partial_h2 = torch.from_numpy(partial_h2).to(torch.float).to(device)\n",
    "\n",
    "                        output1.backward(-0.1*partial_h1, retain_graph=True)\n",
    "                        output2.backward(-0.1*partial_h2, retain_graph=True)\n",
    "                        predict_loss.backward()\n",
    "\n",
    "                        optimizer_model1.step()\n",
    "                        optimizer_model2.step()\n",
    "                        optimizer_classifier.step()\n",
    "                    # for every epoch, evaluate the model on both train and test set\n",
    "                    mymodel.eval()\n",
    "                    predict_out_train, cca_loss_train, _, _, _, _, _, _  = mymodel(train_eeg, train_eye)\n",
    "                    predict_loss_train = class_loss_func(predict_out_train, train_label)\n",
    "                    accuracy_train = np.sum(np.argmax(predict_out_train.detach().cpu().numpy(), axis=1) == train_label.detach().cpu().numpy()) / predict_out_train.shape[0]\n",
    "\n",
    "                    predict_out_test, cca_loss_test, output_1_test, output_2_test, _, _, fused_tensor_test, attention_weight_test  = mymodel(test_eeg, test_eye)\n",
    "                    predict_loss_test = class_loss_func(predict_out_test, test_label)\n",
    "                    accuracy_test = np.sum(np.argmax(predict_out_test.detach().cpu().numpy(), axis=1) == test_label.detach().cpu().numpy()) / predict_out_test.shape[0]\n",
    "\n",
    "                    if accuracy_test > best_test_res['acc']:\n",
    "                        best_test_res['acc'] = accuracy_test\n",
    "                        best_test_res['layer_size'] = layer_sizes\n",
    "                        best_test_res['predict_proba'] = predict_out_test.detach().cpu().data\n",
    "                        best_test_res['fused_feature'] = fused_tensor_test\n",
    "                        best_test_res['transformed_eeg'] = output_1_test.detach().cpu().data\n",
    "                        best_test_res['transformed_eye'] = output_2_test.detach().cpu().data\n",
    "                        best_test_res['alpha'] = attention_weight_test\n",
    "                        best_test_res['true_label'] = test_label.detach().cpu().data\n",
    "\n",
    "                    print('Epoch: {} -- Train CCA loss is: {} -- Train loss: {} -- Train accuracy: {}'.format(epoch, cca_loss_train, predict_loss_train.data, accuracy_train))\n",
    "                    print('Epoch: {} -- Test CCA loss is: {} -- Test loss: {} -- Test accuracy: {}'.format(epoch, cca_loss_test, predict_loss_test.data, accuracy_test))\n",
    "                    print('\\n')\n",
    "                    logging.info('\\tTrain\\t{}\\t{}\\t{}\\t{}'.format(epoch, cca_loss_train, predict_loss_train.data, accuracy_train))\n",
    "                    logging.info('\\tTest\\t{}\\t{}\\t{}\\t{}'.format(epoch, cca_loss_test, predict_loss_test.data, accuracy_test))\n",
    "\n",
    "                pickle.dump(best_test_res, open( os.path.join('./', f_id[:-8]+'_'+str(hyper_choose)), 'wb'  ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1199, 33), (1199, 310))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eye.shape, train_eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "febca9d0ea2080a8720a7456a51ecee5a4c9baf10f0eeace0556f9db084a2e46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
